{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Question 1 : What is Information Gain, and how is it used in Decision Trees?\n",
        "\n",
        "Ans: Information Gain measures the reduction in uncertainty (impurity) about the target variable after splitting the dataset based on a particular feature.\n",
        "\n",
        "arent entropy = 0.94\n",
        "\n",
        "After splitting on Weather, weighted entropy = 0.69\n",
        "\n",
        "ùêºùê∫=0.94‚àí0.69=0.25\n",
        "\n",
        "‚û° This means Weather reduces uncertainty by 0.25 bits, making it a good split."
      ],
      "metadata": {
        "id": "ruVN3Q0uz898"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 2: What is the difference between Gini Impurity and Entropy?\n",
        "\n",
        "1. Gini Impurity\n",
        "Gini Impurity measures the probability of incorrectly classifying a randomly chosen data point if it were labeled according to the class distribution.\n",
        "\n",
        "Gini=1‚àí‚àëpi2‚Äã\n",
        "\n",
        "2. Entropy\n",
        "\n",
        "Entropy measures the amount of uncertainty or randomness in the data.\n",
        "\n",
        "Entropy=‚àí‚àëpi‚Äãlog2‚Äã(pi‚Äã)"
      ],
      "metadata": {
        "id": "xoqWzovN0lZc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 3:What is Pre-Pruning in Decision Trees?\n",
        "\n",
        "Ans: Pre-pruning (also called early stopping) is a technique used in Decision Trees to stop the tree from growing too deep during training, in order to prevent overfitting."
      ],
      "metadata": {
        "id": "hdXOEUjk1BzE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 4:Write a Python program to train a Decision Tree Classifier using Gini\n",
        "Impurity as the criterion and print the feature importances (practical).\n"
      ],
      "metadata": {
        "id": "ZZ0rihkG1LYx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load sample dataset (Iris)\n",
        "data = load_iris()\n",
        "X = pd.DataFrame(data.data, columns=data.feature_names)\n",
        "y = data.target\n",
        "\n",
        "# Split data into train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, random_state=42\n",
        ")\n",
        "\n",
        "# Train Decision Tree Classifier using Gini Impurity\n",
        "model = DecisionTreeClassifier(criterion=\"gini\", random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Get feature importances\n",
        "importances = model.feature_importances_\n",
        "\n",
        "# Display feature importances\n",
        "feature_importance_df = pd.DataFrame({\n",
        "    \"Feature\": X.columns,\n",
        "    \"Importance\": importances\n",
        "}).sort_values(by=\"Importance\", ascending=False)\n",
        "\n",
        "print(\"Feature Importances:\")\n",
        "print(feature_importance_df)"
      ],
      "metadata": {
        "id": "nz18pCuO1dvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 5: What is a Support Vector Machine (SVM)?\n",
        "\n",
        "Ans: A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks. It works by finding the optimal decision boundary (hyperplane) that best separates data points of different classes."
      ],
      "metadata": {
        "id": "jIEbb4RA1hFC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 6: What is the Kernel Trick in SVM?\n",
        "\n",
        "The Kernel Trick is a technique used in Support Vector Machines to handle non-linearly separable data by implicitly mapping it into a higher-dimensional feature space, where a linear separator can be found‚Äîwithout explicitly computing that transformation."
      ],
      "metadata": {
        "id": "bU4FPzUS2HR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 7: Write a Python program to train two SVM classifiers with Linear and RBF\n",
        "kernels on the Wine dataset, then compare their accuracies.\n",
        "Hint:Use SVC(kernel='linear') and SVC(kernel='rbf'), then compare accuracy scores after fitting\n",
        "on the same dataset."
      ],
      "metadata": {
        "id": "_MNeVY2_2WS7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_wine\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load Wine dataset\n",
        "data = load_wine()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Feature scaling (important for SVM)\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Train Linear SVM\n",
        "svm_linear = SVC(kernel='linear', random_state=42)\n",
        "svm_linear.fit(X_train_scaled, y_train)\n",
        "y_pred_linear = svm_linear.predict(X_test_scaled)\n",
        "\n",
        "# Train RBF SVM\n",
        "svm_rbf = SVC(kernel='rbf', random_state=42)\n",
        "svm_rbf.fit(X_train_scaled, y_train)\n",
        "y_pred_rbf = svm_rbf.predict(X_test_scaled)\n",
        "\n",
        "# Accuracy scores\n",
        "linear_accuracy = accuracy_score(y_test, y_pred_linear)\n",
        "rbf_accuracy = accuracy_score(y_test, y_pred_rbf)\n",
        "\n",
        "print(\"Linear SVM Accuracy:\", linear_accuracy)\n",
        "print(\"RBF SVM Accuracy:\", rbf_accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q2Mi56Ii2ZSK",
        "outputId": "9a588179-23d1-4de7-ffb5-a16f6504ad10"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear SVM Accuracy: 0.9629629629629629\n",
            "RBF SVM Accuracy: 0.9814814814814815\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 8: What is the Na√Øve Bayes classifier, and why is it called \"Na√Øve\"?\n",
        "\n",
        "Ans: The Na√Øve Bayes classifier is a supervised, probabilistic machine learning algorithm based on Bayes‚Äô Theorem. It is widely used for classification tasks, especially in text classification and spam detection.\n",
        "\n",
        "The Na√Øve Bayes classifier is a probabilistic classification algorithm based on Bayes‚Äô Theorem. It is called ‚Äúna√Øve‚Äù because it assumes that all features are independent of each other given the class label."
      ],
      "metadata": {
        "id": "BnP0rey02gZ6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 9: Explain the differences between Gaussian Na√Øve Bayes, Multinomial Na√Øve\n",
        "Bayes, and Bernoulli Na√Øve Bayes"
      ],
      "metadata": {
        "id": "xm0NTIpC20nr"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Na√Øve Bayes classifiers are probabilistic models based on Bayes‚Äô theorem with the assumption of conditional independence among features. The three common variants differ mainly in the type of data they model.\n",
        "\n",
        "1. Gaussian Na√Øve Bayes\n",
        "What it Assumes\n",
        "\n",
        "Features are continuous\n",
        "\n",
        "Feature values follow a Gaussian (normal) distribution\n",
        "\n",
        "2. Multinomial Na√Øve Bayes\n",
        "What it Assumes\n",
        "\n",
        "Features are discrete counts\n",
        "\n",
        "Data follows a multinomial distribution\n",
        "\n",
        "3. Bernoulli Na√Øve Bayes\n",
        "What it Assumes\n",
        "\n",
        "Features are binary (0 or 1)\n",
        "\n",
        "Data follows a Bernoulli distribution"
      ],
      "metadata": {
        "id": "vH-ZkIvD23av"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Question 10: Breast Cancer Dataset\n",
        "Write a Python program to train a Gaussian Na√Øve Bayes classifier on the Breast Cancer\n",
        "dataset and evaluate accuracy.\n",
        "Hint:Use GaussianNB() from sklearn.naive_bayes and the Breast Cancer dataset from\n",
        "sklearn.datasets."
      ],
      "metadata": {
        "id": "VSx3h6KO3FlE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data\n",
        "y = data.target\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.3, random_state=42, stratify=y\n",
        ")\n",
        "\n",
        "# Train Gaussian Naive Bayes classifier\n",
        "model = GaussianNB()\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Accuracy evaluation\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "print(\"Gaussian Naive Bayes Accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "q-jjNVkh3PHo",
        "outputId": "814b9354-78be-460d-c9f8-544dd69360c0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Gaussian Naive Bayes Accuracy: 0.9473684210526315\n"
          ]
        }
      ]
    }
  ]
}